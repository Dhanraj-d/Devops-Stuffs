# Install NVIDIA Container Toolkit

curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \
  | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
  | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \
  | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
🧠 3. Configure Docker to use NVIDIA runtime

sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
🚀 4. Run Ollama container with GPU

docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
🦙 5. Run a model inside the container

docker exec -it ollama ollama run llama3
This will download and run the llama3 model.

🌐 6. Test API access
From the same EC2:


curl http://localhost:11434
From your local machine (replace IP):

curl http://<your-ec2-public-ip>:11434
 Ensure port 11434 is open in Security Group for inbound access.

Let me know if you want to expose Ollama via a domain (like llm.yourdomain.com) with HTTPS too.
